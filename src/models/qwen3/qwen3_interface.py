# src/models/qwen3/qwen3_interface.py
"""Interface pour interagir avec le mod√®le Qwen3 via Ollama.

Cette interface est sp√©cialis√©e dans l'analyse de Sp√©cifications Fonctionnelles D√©taill√©es (SFD)
avec un support pour un grand contexte (32K tokens), optimis√©e pour la g√©n√©ration
de sc√©narios de test sur CPU. Elle int√®gre un disjoncteur (circuit breaker)
pour g√©rer les d√©faillances temporaires d'Ollama et un cache en m√©moire.
"""
from __future__ import annotations

import logging
from typing import Dict, Optional, Any

import aiohttp

from src.modules.psychodesign.personality_evolution import PersonalityEvolution
from src.models.sfd_models import SFDAnalysisRequest
from src.utils.retry_handler import CircuitBreaker

logger = logging.getLogger(__name__)


class Qwen3OllamaInterface:
    """Interface asynchrone pour communiquer avec Qwen3 via Ollama."""

    def __init__(self,
                 model_name: str = "qwen3-sfd-analyzer",
                 ollama_host: str = "http://localhost:11434",
                 timeout: int = 120,
                 cache_enabled: bool = True,
                 failure_threshold: int = 5,
                 recovery_timeout: int = 60):
        """Initialise l'interface Qwen3.

        Args:
            model_name: Le nom du mod√®le Qwen3 √† utiliser dans Ollama.
            ollama_host: L'URL de l'h√¥te Ollama.
            timeout: Le d√©lai d'attente maximal pour les requ√™tes HTTP en secondes.
            cache_enabled: Active ou d√©sactive le cache en m√©moire pour les r√©sultats d'analyse.
            failure_threshold: Nombre d'√©checs cons√©cutifs avant que le disjoncteur ne s'ouvre.
            recovery_timeout: Temps en secondes avant que le disjoncteur ne tente de se refermer.
        """
        self.model_name = model_name
        self.ollama_host = ollama_host
        self.timeout = timeout
        self.cache_enabled = cache_enabled
        self.session: Optional[aiohttp.ClientSession] = None
        self.adapter_path: Optional[str] = None
        self.personality_evolver = PersonalityEvolution()
        self.circuit_breaker = CircuitBreaker(failure_threshold, recovery_timeout)

        # Cache en m√©moire pour √©viter les re-analyses co√ªteuses.
        self.cache: Dict[str, Dict] = {}

        # Configuration sp√©cifique √† Qwen3 pour l'inf√©rence.
        self.default_params = {
            "temperature": 0.7, # Contr√¥le la cr√©ativit√© de la r√©ponse.
            "top_p": 0.9,       # Contr√¥le la diversit√© de la r√©ponse.
            "top_k": 40,        # Limite le nombre de jetons consid√©r√©s pour la pr√©diction.
            "repeat_penalty": 1.1, # P√©nalise la r√©p√©tition de jetons.
            "num_ctx": 32768,   # Taille du contexte en jetons (pour les documents longs).
            "num_predict": 4096 # Nombre maximal de jetons √† g√©n√©rer.
        }

    async def initialize(self):
        """Initialise la session HTTP et v√©rifie la disponibilit√© du mod√®le dans Ollama."

        Raises:
            RuntimeError: Si le mod√®le n'est pas disponible dans Ollama apr√®s v√©rification.
        """
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.timeout)
        )

        # V√©rifie que le mod√®le est disponible avant de continuer.
        if not await self._check_model():
            raise RuntimeError(f"Le mod√®le `{self.model_name}` n'est pas disponible dans Ollama.")

        # Pr√©charge le mod√®le en m√©moire pour r√©duire la latence au premier appel.
        await self._preload_model()

        # Charge le chemin du dernier adaptateur de personnalit√© LoRA si disponible.
        self.load_latest_adapter()

        logger.info(f"Interface Qwen3 initialis√©e avec le mod√®le `{self.model_name}`.")

    async def _check_model(self) -> bool:
        """V√©rifie la disponibilit√© du mod√®le Qwen3 dans Ollama via le circuit breaker."""

        async def _do_check():
            """Fonction interne pour la v√©rification du mod√®le."""
            async with self.session.get(f"{self.ollama_host}/api/tags") as resp:
                resp.raise_for_status()  # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx.
                data = await resp.json()
                models = [m['name'] for m in data.get('models', [])]
                return any(self.model_name in model for model in models)

        try:
            return await self.circuit_breaker.call(_do_check)
        except aiohttp.ClientError as e:
            logger.error(f"Erreur r√©seau lors de la v√©rification du mod√®le : {e}")
            return False
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la v√©rification du mod√®le : {e}")
            return False

    async def _preload_model(self):
        """Pr√©charge le mod√®le en m√©moire Ollama pour r√©duire la latence des requ√™tes futures."""
        logger.info(f"Pr√©chargement du mod√®le Qwen3 ({self.model_name})...")

        async def _do_preload():
            """Fonction interne pour le pr√©chargement du mod√®le."""
            payload = {
                "model": self.model_name,
                "prompt": "Bonjour", # Un prompt minimal pour d√©clencher le chargement.
                "stream": False,
                "options": {"num_predict": 1}
            }
            async with self.session.post(
                    f"{self.ollama_host}/api/generate",
                    json=payload
            ) as resp:
                resp.raise_for_status()
                await resp.json()

        try:
            await self.circuit_breaker.call(_do_preload)
            logger.info("Mod√®le Qwen3 pr√©charg√© avec succ√®s.")
        except aiohttp.ClientError as e:
            logger.warning(f"√âchec du pr√©chargement du mod√®le (non critique) : {e}")
        except Exception as e:
            logger.warning(f"√âchec inattendu du pr√©chargement du mod√®le (non critique) : {e}")

    def load_latest_adapter(self):
        """Charge le chemin du dernier adaptateur LoRA de personnalit√© disponible."

        Si un adaptateur est trouv√©, son chemin est stock√© pour √™tre utilis√©
        lors de la construction des prompts.
        """
        latest_adapter_path = self.personality_evolver.get_latest_adapter()
        if latest_adapter_path and latest_adapter_path.exists():
            self.adapter_path = str(latest_adapter_path)
            logger.info(f"üß† Adaptateur de personnalit√© LoRA charg√© : {self.adapter_path}")
        else:
            logger.info("üß† Aucun adaptateur de personnalit√© trouv√©, utilisation du mod√®le de base Qwen3.")

    async def analyze_sfd(
        self,
        request: SFDAnalysisRequest,
        use_cache: bool = True
    ) -> Dict:
        """Analyse une Sp√©cification Fonctionnelle D√©taill√©e (SFD) et extrait les sc√©narios de test.

        Args:
            request: Un objet `SFDAnalysisRequest` contenant le contenu de la SFD
                     et le type d'extraction souhait√©.
            use_cache: Si True, le r√©sultat sera r√©cup√©r√© du cache si disponible et mis en cache apr√®s traitement.

        Returns:
            Un dictionnaire contenant les sc√©narios extraits et d'autres m√©tadonn√©es d'analyse.

        Raises:
            aiohttp.ClientError: En cas d'erreur de communication avec Ollama.
            Exception: En cas d'erreur inattendue lors de l'analyse.
        """
        cache_key = self._generate_cache_key(request.content, request.extraction_type)

        # V√©rifie le cache en m√©moire si activ√©.
        if use_cache and self.cache_enabled and cache_key in self.cache:
            logger.info("R√©sultat d'analyse SFD trouv√© dans le cache en m√©moire.")
            return self.cache[cache_key]

        # Construit le prompt pour Qwen3 en fonction du type d'extraction demand√©.
        prompt = self._build_prompt(request)

        try:
            # Utilise le circuit breaker pour l'appel √† l'API Ollama.
            async with self.session.post(
                    f"{self.ollama_host}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": prompt,
                        **self.default_params
                    }
            ) as resp:
                resp.raise_for_status()
                data = await resp.json()
                result = self._parse_response(data)
                
                # Met en cache le r√©sultat si activ√©.
                if use_cache and self.cache_enabled:
                    self.cache[cache_key] = result
                return result
        except aiohttp.ClientError as e:
            logger.error(f"Erreur r√©seau lors de la g√©n√©ration de texte par Qwen3 : {e}")
            raise
        except Exception as e:
            logger.error(f"Erreur inattendue lors de la g√©n√©ration de texte par Qwen3 : {e}")
            raise

    async def close(self) -> None:
        """Ferme la session HTTP asynchrone."""
        if self.session and not self.session.closed:
            await self.session.close()

    @staticmethod
    def _generate_cache_key(content: str, extraction_type: str) -> str:
        """G√©n√®re une cl√© de cache unique bas√©e sur le contenu de la SFD et le type d'extraction."""
        # Utilise un hachage du contenu pour une cl√© compacte et unique.
        return f"{hash(content)}_{extraction_type}"

    @staticmethod
    def _build_prompt(request: SFDAnalysisRequest) -> str:
        """Construit le prompt pour Qwen3 en fonction du type d'extraction demand√©."""
        # Ce prompt est un exemple et devrait √™tre affin√© pour Qwen3.
        # Il devrait inclure des instructions claires sur le format de sortie attendu (JSON).
        if request.extraction_type == "complete":
            instruction = "Extrayez tous les sc√©narios de test d√©taill√©s de la sp√©cification suivante, incluant titre, objectif, criticit√©, pr√©conditions, √©tapes, r√©sultat attendu et donn√©es de test. Retournez le tout au format JSON."
        elif request.extraction_type == "summary":
            instruction = "Fournissez un r√©sum√© des principaux sc√©narios de test de la sp√©cification suivante. Retournez le tout au format JSON."
        elif request.extraction_type == "critical_only":
            instruction = "Identifiez et extrayez uniquement les sc√©narios de test critiques de la sp√©cification suivante. Retournez le tout au format JSON."
        else:
            instruction = "Extrayez les sc√©narios de test de la sp√©cification suivante au format JSON."

        return f"""
Vous √™tes un expert en analyse de sp√©cifications fonctionnelles. Votre t√¢che est d'analyser la SFD fournie et d'en extraire les sc√©narios de test.

Instruction: {instruction}

Sp√©cification Fonctionnelle D√©taill√©e:
```
{request.content}
```

R√©ponse JSON:
"""

    @staticmethod
    def _parse_response(data: Dict) -> Dict:
        """Analyse la r√©ponse JSON brute d'Ollama et extrait les sc√©narios de test."

        Args:
            data: Le dictionnaire de r√©ponse brut d'Ollama.

        Returns:
            Un dictionnaire contenant les sc√©narios extraits et d'autres informations.
        """
        # La r√©ponse d'Ollama est dans `data["response"]`.
        # Il faut ensuite parser cette cha√Æne qui devrait √™tre du JSON.
        raw_response_text = data.get("response", "{}")
        try:
            parsed_json = json.loads(raw_response_text)
            # Assurez-vous que la structure correspond √† ce que vous attendez.
            return {"scenarios": parsed_json.get("scenarios", []), "raw_output": raw_response_text}
        except json.JSONDecodeError as e:
            logger.error(f"Erreur de parsing JSON de la r√©ponse Qwen3 : {e}. R√©ponse brute : {raw_response_text[:200]}...")
            return {"scenarios": [], "error": f"Parsing JSON √©chou√©: {e}", "raw_output": raw_response_text}


# ------------------------------------------------------------------
# D√©monstration (exemple d'utilisation)
# ------------------------------------------------------------------
if __name__ == "__main__":
    async def demo():
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # Assurez-vous qu'Ollama est lanc√© et que le mod√®le 'qwen3-sfd-analyzer' est pull.
        # ollama pull qwen3-sfd-analyzer

        interface = Qwen3OllamaInterface()
        try:
            await interface.initialize()

            sfd_content = """
            Sp√©cification Fonctionnelle: Module de Connexion

            1. Sc√©nario: Connexion r√©ussie
            - √âtapes: Entrer un email et un mot de passe valides, cliquer sur 'Se connecter'.
            - R√©sultat attendu: Redirection vers le tableau de bord.

            2. Sc√©nario: Mot de passe incorrect
            - √âtapes: Entrer un email valide et un mot de passe incorrect, cliquer sur 'Se connecter'.
            - R√©sultat attendu: Message d'erreur 'Mot de passe incorrect'.
            """
            sfd_request = SFDAnalysisRequest(content=sfd_content, extraction_type="complete")

            print("\n--- Analyse de la SFD (compl√®te) ---")
            analysis_result = await interface.analyze_sfd(sfd_request)
            print(f"R√©sultat de l'analyse : {json.dumps(analysis_result, indent=2, ensure_ascii=False)}")

            # D√©monstration du cache.
            print("\n--- Analyse de la SFD (depuis le cache) ---")
            analysis_result_cached = await interface.analyze_sfd(sfd_request, use_cache=True)
            print(f"R√©sultat de l'analyse (depuis cache) : {json.dumps(analysis_result_cached, indent=2, ensure_ascii=False)}")

        except RuntimeError as e:
            logging.error(f"Erreur d'initialisation de l'interface Qwen3 : {e}")
        except Exception as e:
            logging.error(f"Erreur lors de l'analyse de la SFD : {e}")
        finally:
            await interface.close()

    asyncio.run(demo())